# app.py
import os
import time
import requests
import numpy as np
import pandas as pd
import streamlit as st

from typing import List, Dict, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

st.set_page_config(
    page_title="ìë£Œ ì†ŒìŠ¤ íë ˆì´í„° MyGPT (Streamlit)",
    page_icon="ğŸ”",
    layout="wide"
)

# -----------------------------
# 0) ì¹´í…Œê³ ë¦¬ ë™ì˜ì–´ ì‚¬ì „ (ì´ìƒì˜ ë¡œì§ ë°˜ì˜)
# -----------------------------
CATEGORY_SYNONYMS: Dict[str, List[str]] = {
    "ì •ë¶€Â·ê³µê³µ ë°ì´í„°": ["ì •ë¶€","ê³µê³µ","í†µê³„","ìƒê¶Œ","í–‰ì •","ì •ì±…","KDI","í†µê³„ì²­","ê³µê³µë°ì´í„°","KOSIS","PRISM","NKIS"],
    "í•´ì™¸ ë™í–¥": ["í•´ì™¸","ê¸€ë¡œë²Œ","êµ­ì œ","OECD","Deloitte","PwC","Accenture","Ipsos","Consumer Reports","êµ­ì™¸"],
    "íšŒì‚¬ ë™í–¥": ["ê¸°ì—…","ê²½ìŸì‚¬","IR","ìŠ¤íƒ€íŠ¸ì—…","ì¦ê¶Œ","ì• ë„ë¦¬ìŠ¤íŠ¸","ì»¨ì„¼ì„œìŠ¤","í•œê²½","THE VC","DART","ì¬ë¬´"],
    "ì‚°ì—… ë™í–¥": ["ì‚°ì—…","ì‹œì¥","íŠ¸ë Œë“œ","ì½˜í…ì¸ ","ê´‘ê³ ","IT","ë¬´ì—­","í•€í…Œí¬","í”„ëœì°¨ì´ì¦ˆ","ì‹í’ˆ","ë¯¸ë””ì–´"],
    "ë§ˆì¼€íŒ… ì¡°ì‚¬": ["ì¡°ì‚¬","ì„œë² ì´","ì—¬ë¡ ","ì»¨ìŠˆë¨¸","ê°¤ëŸ½","ì˜¤í”ˆì„œë² ì´","ì¹¸íƒ€","TrendWatching","íŒ¨ë„"],
    "í•™ìˆ ": ["í•™ìˆ ","ë…¼ë¬¸","ìŠ¤ì¹¼ë¼","ì•„ì¹´ë°ë¯¹","êµ­íšŒë„ì„œê´€","í•™ìœ„ë…¼ë¬¸"],
    "ë‰´ìŠ¤ë ˆí„°": ["ë‰´ìŠ¤ë ˆí„°","ì–´í”¼í‹°","ë‰´ë‹‰","ìºë¦¿","ìŠ¤íƒ€íŠ¸ì—… ìœ„í´ë¦¬","ì½˜í…íƒ€","ì–´ê±°ìŠ¤íŠ¸","ìš”ì•½"]
}

DEFAULT_TOP_K = 10

# -----------------------------
# 1) ë°ì´í„° ë¡œë”©
# -----------------------------
@st.cache_data(show_spinner=False)
def load_sources(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì‚¬
    need_cols = {"category","site_name","url","short_desc","tags"}
    miss = need_cols - set(df.columns)
    if miss:
        raise ValueError(f"CSVì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {sorted(miss)}")
    # ê²°ì¸¡ì¹˜ ë°©ì–´
    for c in ["category","site_name","url","short_desc","tags"]:
        df[c] = df[c].fillna("")
    return df

def ensure_sources() -> Tuple[pd.DataFrame, str]:
    # ê¸°ë³¸ íŒŒì¼ëª…
    default_path = "sources.csv"
    if os.path.exists(default_path):
        return load_sources(default_path), default_path
    # ì•ˆë‚´ìš© ìƒ˜í”Œ(ìµœì†Œ ì…‹)
    sample = pd.DataFrame([
        ["ì‚°ì—… ë™í–¥","KOTRA í•´ì™¸ì‹œì¥ë‰´ìŠ¤","https://news.kotra.or.kr","êµ­ê°€ë³„ ì‚°ì—… ë™í–¥/ìˆ˜ì¶œ íŠ¸ë Œë“œ","í•´ì™¸,ì‚°ì—…,ë¬´ì—­,ì‹œì¥"],
        ["ì •ë¶€Â·ê³µê³µ ë°ì´í„°","ê³µê³µë°ì´í„° í¬í„¸","https://www.data.go.kr","ê³µê³µë°ì´í„°Â·API ì œê³µ í¬í„¸","ê³µê³µë°ì´í„°,API,ìë£Œ"],
        ["íšŒì‚¬ ë™í–¥","DART ì „ìê³µì‹œ","https://dart.fss.or.kr","ìƒì¥ì‚¬ ê³µì‹œ/IR ìë£Œ","IR,ê³µì‹œ,ì¬ë¬´"],
        ["í•™ìˆ ","êµ¬ê¸€ ìŠ¤ì¹¼ë¼","https://scholar.google.com","í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰","ë…¼ë¬¸,ê²€ìƒ‰,í•™ìˆ "],
        ["ë‰´ìŠ¤ë ˆí„°","ë‰´ë‹‰","https://newneek.co","ì‹œì‚¬/ìš©ì–´ ìš”ì•½í˜• ë‰´ìŠ¤ë ˆí„°","ë‰´ìŠ¤,ìš”ì•½,íŠ¸ë Œë“œ"]
    ], columns=["category","site_name","url","short_desc","tags"])
    return sample, "(ì„ì‹œ ìƒ˜í”Œ ë©”ëª¨ë¦¬ ë¡œë”©)"

# -----------------------------
# 2) í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ & ë²¡í„°í™”
# -----------------------------
@st.cache_data(show_spinner=False)
def build_vectorizer(df: pd.DataFrame):
    corpus = (df["site_name"].astype(str) + " " +
              df["short_desc"].astype(str) + " " +
              df["tags"].astype(str) + " " +
              df["category"].astype(str)
             ).str.lower().tolist()

    vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=1)
    X = vectorizer.fit_transform(corpus)
    return vectorizer, X

def expand_query_with_synonyms(q: str) -> Tuple[str, List[str]]:
    q_low = q.lower()
    boost_categories = []
    for cat, syns in CATEGORY_SYNONYMS.items():
        if any(s.lower() in q_low for s in syns) or cat.lower() in q_low:
            boost_categories.append(cat)
    return q, boost_categories

# -----------------------------
# 3) ìœ ì‚¬ë„ + ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜
# -----------------------------
def score_and_rank(
    df: pd.DataFrame,
    vectorizer,
    X,
    query: str,
    manual_boost_cats: List[str] = None,
    auto_boost: bool = True,
    boost_weight: float = 0.25
) -> pd.DataFrame:
    manual_boost_cats = manual_boost_cats or []
    q_expanded, auto_boost_cats = expand_query_with_synonyms(query) if auto_boost else (query, [])
    q_vec = vectorizer.transform([q_expanded.lower()])
    sim = cosine_similarity(q_vec, X).ravel()

    # ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜: ìˆ˜ë™ + ìë™
    boost_set = set(manual_boost_cats) | set(auto_boost_cats)
    if boost_set:
        cat_arr = df["category"].values
        mul = np.ones_like(sim)
        for cat in boost_set:
            mul = mul * np.where(cat_arr == cat, 1.0 + boost_weight, 1.0)
        sim = sim * mul

    out = df.copy()
    out["score"] = sim
    # tie-breaker: score desc, site_name asc
    out = out.sort_values(by=["score","site_name"], ascending=[False, True], kind="mergesort")
    return out

# -----------------------------
# 4) URL ì²´í¬(ì˜µì…˜)
# -----------------------------
def check_url_status(url: str, timeout=5) -> str:
    if not url or not url.startswith(("http://","https://")):
        return "N/A"
    try:
        r = requests.head(url, allow_redirects=True, timeout=timeout)
        return str(r.status_code)
    except Exception:
        return "ERR"

# -----------------------------
# 5) UI
# -----------------------------
def main():
    st.title("ğŸ” ìë£Œ ì†ŒìŠ¤ íë ˆì´í„° MyGPT (Streamlit ë²„ì „)")
    st.caption("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ë©´, ë¬¸ì„œ( CSV )ì˜ ì¹´í…Œê³ ë¦¬ë³„ ìë£Œ ì¶œì²˜ë¥¼ ì—°ê´€ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•´ ë³´ì—¬ì¤ë‹ˆë‹¤.")

    with st.sidebar:
        st.header("âš™ï¸ ì˜µì…˜")
        uploaded = st.file_uploader("CSV ì—…ë¡œë“œ (sources.csv ì»¬ëŸ¼ ê³ ì •)", type=["csv"])
        if uploaded is not None:
            df = pd.read_csv(uploaded)
            st.success("CSV ì—…ë¡œë“œ ì™„ë£Œ")
        else:
            df, path = ensure_sources()
            if path.endswith(".csv"):
                st.info(f"CSV ì‚¬ìš©: {path}")
            else:
                st.warning("ë¡œì»¬ sources.csvê°€ ì—†ì–´ ìƒ˜í”Œë¡œ ë™ì‘í•©ë‹ˆë‹¤. ì‹¤ì œ ì‚¬ìš©ì‹œ sources.csvë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")

        # ë²¡í„°í™” ì¤€ë¹„
        try:
            vectorizer, X = build_vectorizer(df)
        except Exception as e:
            st.error(f"CSV í˜•ì‹ ì˜¤ë¥˜: {e}")
            st.stop()

        # ì‚¬ìš©ì ì˜µì…˜
        group_by_cat = st.toggle("ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹ í‘œì‹œ", value=True)
        top_k = st.slider("ì¶œë ¥ ê°œìˆ˜(ì „ì²´ ê¸°ì¤€)", min_value=5, max_value=50, value=DEFAULT_TOP_K, step=5)
        top_per_cat = st.slider("ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ N (ê·¸ë£¹ í‘œì‹œ ì‹œ ì ìš©)", 1, 10, 5) if group_by_cat else None

        manual_cats = st.multiselect(
            "ê°€ì¤‘ì¹˜ë¥¼ ì¤„ ì¹´í…Œê³ ë¦¬ ì„ íƒ(ì„ íƒ)",
            sorted(df["category"].dropna().unique().tolist())
        )
        boost_weight = st.slider("ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜(%)", 0, 100, 25, step=5) / 100.0

        do_auto_boost = st.toggle("ë™ì˜ì–´ ê¸°ë°˜ ìë™ ì¹´í…Œê³ ë¦¬ ê°ì§€", value=True)
        do_url_check = st.toggle("URL ìœ íš¨ì„± ê²€ì‚¬(ëŠë¦´ ìˆ˜ ìˆìŒ)", value=False)
        show_table = st.toggle("ì›ë³¸ í…Œì´ë¸” ë³´ê¸°", value=False)

    # ë©”ì¸ ì…ë ¥
    query = st.text_input("ğŸ” ì£¼ì œ/í‚¤ì›Œë“œ ì…ë ¥", placeholder="ì˜ˆ: í•€í…Œí¬ ê²°ì œ ë°ì´í„° íŠ¸ë Œë“œ, í”„ëœì°¨ì´ì¦ˆ ë²•ë¥ , ìƒê¶Œ ë¶„ì„ ...")
    search_btn = st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary", use_container_width=True)

    if show_table:
        with st.expander("ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
            st.dataframe(df, use_container_width=True, height=300)

    if not search_btn and not query:
        st.info("ì¢Œì¸¡ì—ì„œ ì˜µì…˜ì„ ì¡°ì •í•˜ê³ , ìœ„ ì…ë ¥ì°½ì— í‚¤ì›Œë“œë¥¼ ì…ë ¥í•œ ë’¤ **ê²€ìƒ‰ ì‹¤í–‰**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        return

    if not query:
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    # ì ìˆ˜ ê³„ì‚°
    ranked = score_and_rank(
        df, vectorizer, X,
        query=query,
        manual_boost_cats=manual_cats,
        auto_boost=do_auto_boost,
        boost_weight=boost_weight
    )

    # ìƒìœ„ N í•„í„°
    ranked_top = ranked.head(top_k)

    # (ì˜µì…˜) URL ì²´í¬
    if do_url_check:
        with st.status("URL ìƒíƒœ í™•ì¸ ì¤‘...", expanded=False):
            ranked_top["url_status"] = ranked_top["url"].apply(check_url_status)
            time.sleep(0.3)

    # í‘œì¤€ ì¶œë ¥ í¬ë§·
    st.subheader("ğŸ” ê²€ìƒ‰ ê²°ê³¼")
    st.caption(f"í‚¤ì›Œë“œ: **{query}** | ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜: {manual_cats or 'ì—†ìŒ'} | ìë™ ê°ì§€: {do_auto_boost} | ê°€ì¤‘ì¹˜ê³„ìˆ˜: {boost_weight:.2f}")

    if group_by_cat:
        blocks = []
        for cat, sub in ranked_top.groupby("category", sort=False):
            sub = sub.copy()
            if top_per_cat:
                sub = sub.head(top_per_cat)
            blocks.append((cat, sub))

        for cat, sub in blocks:
            st.markdown(f"### ğŸ“‚ {cat}")
            for i, row in sub.iterrows():
                score_pct = f"{row['score']*100:,.1f}%"
                url = row["url"] if row["url"] else "(URL ì—†ìŒ)"
                desc = row["short_desc"] if row["short_desc"] else ""
                line = f"**â€¢ {row['site_name']}** â€” {url} â€” {desc}  \n(ì—°ê´€ë„: {score_pct})"
                if do_url_check:
                    line += f"  \nURL ìƒíƒœ: `{row.get('url_status','N/A')}`"
                st.markdown(line)
            st.divider()
    else:
        for i, row in ranked_top.iterrows():
            score_pct = f"{row['score']*100:,.1f}%"
            url = row["url"] if row["url"] else "(URL ì—†ìŒ)"
            desc = row["short_desc"] if row["short_desc"] else ""
            line = f"**â€¢ [{row['category']}] {row['site_name']}** â€” {url} â€” {desc}  \n(ì—°ê´€ë„: {score_pct})"
            if do_url_check:
                line += f"  \nURL ìƒíƒœ: `{row.get('url_status','N/A')}`"
            st.markdown(line)

    # ë‹¤ìš´ë¡œë“œ
    csv_bytes = ranked_top.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "â¬‡ï¸ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv_bytes,
        file_name="search_results.csv",
        mime="text/csv",
        use_container_width=True
    )

    st.success("ì™„ë£Œ! ì˜µì…˜ì„ ë°”ê¿” ë‹¤ì‹œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”.")

if __name__ == "__main__":
    main()
